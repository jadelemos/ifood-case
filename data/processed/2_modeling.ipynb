{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73cc1130-1c06-497f-b7aa-565c4ca60aab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8aa0450f-e2c1-4434-b75d-4c559d41addc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Inicialização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ab22c40-ad0f-4f93-8a14-0de88f8fa2c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Carregando dependências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4deb6c1-0662-4c9b-b96c-28c4d33f8bcd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Utils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "\n",
    "# Modelo\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lightgbm import LGBMClassifier\n",
    "import lightgbm as lgb\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer\n",
    "from sklearn.metrics import accuracy_score, recall_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15099201-c039-470a-a5e6-0c3761dace11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4d39119-d2d9-4536-85fb-f1fc3ab5d4d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def cria_conjuntos(\n",
    "    df: pd.DataFrame(),\n",
    "):\n",
    "    '''\n",
    "    Info:\n",
    "        Função que gera as bases de treino, validacao e teste de tal maneira que nao haja contas em mais de um conjunto.\n",
    "    ----------\n",
    "    Input:\n",
    "        df: Conjunto de dados preprocessado.\n",
    "    ----------\n",
    "    Output:\n",
    "        Conjuntos de treino, validacao e teste\n",
    "    '''\n",
    "    # Separando a base de dados em treino, validação e teste. \n",
    "    # A separação será feita baseada na quantidade de contas em cada conjunto\n",
    "    # As contas em cada conjunto serão exclusivas (não haverá conta no conjunto de teste presente nos demais)\n",
    "\n",
    "    # Substitundo valores nulos em offer_id\n",
    "    df['offer_id'] = df['offer_id'].fillna('No offer')\n",
    "\n",
    "    # Estratificando por oferta\n",
    "    stratify_cols = ['offer_id']\n",
    "    df_strat = (\n",
    "        df[['account_id'] + stratify_cols]\n",
    "        .drop_duplicates(subset='account_id')\n",
    "    )\n",
    "\n",
    "    # Obtendo as contas de treino e teste\n",
    "    train_val_acc, test_acc, train_val_offer, _ = train_test_split(\n",
    "        df_strat[['account_id'] + stratify_cols],\n",
    "        df_strat['offer_id'],\n",
    "        test_size=0.2,\n",
    "        stratify=df_strat[stratify_cols],\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    # Obteando as contas de treino e validação\n",
    "    train_acc, val_acc, _, _ = train_test_split(\n",
    "        train_val_acc[['account_id'] + stratify_cols],\n",
    "        train_val_offer,\n",
    "        test_size=0.2,\n",
    "        stratify=train_val_acc[stratify_cols],\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    # Criando as bases de dados\n",
    "    df_train = df[df.account_id.isin(train_acc.account_id)]\n",
    "    df_val = df[df.account_id.isin(val_acc.account_id)]\n",
    "    df_test = df[df.account_id.isin(test_acc.account_id)]\n",
    "\n",
    "    return df_train, df_val, df_test\n",
    "\n",
    "def transform_data_for_model(\n",
    "    df: pd.DataFrame(),\n",
    "    offer: str,\n",
    "    offers: list=None\n",
    "):\n",
    "    '''\n",
    "    Info:\n",
    "        Funcao utilizada para transformar um conjunto de dados para o modelo especifico.\n",
    "    ----------\n",
    "    Input:\n",
    "        df: Dataset contendo os dados para serem transformados.\n",
    "        offer: ID da oferta para transformar os dados para o modelo especifico.\n",
    "    ----------\n",
    "    Output:\n",
    "        Dataset contendo os dados transformados\n",
    "    '''\n",
    "    # Removendo colunas de ID e as que são relacionadas com o ID da oferta\n",
    "    rem_cols = [\n",
    "        'offer_id', 'account_id',\n",
    "        'discount_value',\n",
    "        'min_value',\n",
    "        'duration',\n",
    "        'num_channels',\n",
    "        'offer_type_bogo', 'offer_type_discount', 'offer_type_informational',\n",
    "        'channel_email', 'channel_mobile', 'channel_social', 'channel_web'\n",
    "    ]\n",
    "\n",
    "    # Transformando o conjunto de dados para o modelo especifico\n",
    "    if(offer != 'No offer'):\n",
    "        return (\n",
    "            df.assign(\n",
    "                target = lambda x: np.where(x.offer_id == offer, 1, 0)\n",
    "            )\n",
    "            .drop(columns = rem_cols)\n",
    "        )\n",
    "    else:\n",
    "\n",
    "        return (\n",
    "            df.assign(\n",
    "                target = lambda x: np.where(x.offer_id.isin(offers), 0, 1)\n",
    "            )\n",
    "            .drop(columns = rem_cols)\n",
    "        )\n",
    "\n",
    "\n",
    "def treinar_modelo_lgbm_bayes(\n",
    "    df: pd.DataFrame(), \n",
    "    offer_id: str, \n",
    "    nome_arquivo: str\n",
    "):\n",
    "    '''\n",
    "    Info:\n",
    "        Treina um modelo LightGBM com otimização Bayesiana dos hiperparâmetros,\n",
    "        avalia o desempenho na base de validação e plota a importância das features.\n",
    "    ---------\n",
    "    Input:\n",
    "        df: dicionário com DataFrames divididos em treino e validação, por offer_id\n",
    "        offer_id: chave usada para acessar os dados específicos de uma oferta\n",
    "        nome_arquivo: nome do arquivo para salvar o modelo (não está sendo salvo no momento)\n",
    "    ---------\n",
    "    Output:\n",
    "        melhor_modelo: modelo treinado com os melhores hiperparâmetros encontrados\n",
    "    '''\n",
    "\n",
    "    # Separa os dados de treino e validação, removendo a coluna 'target' das features\n",
    "    X_train = df[offer_id]['train'].drop(columns=['target'])\n",
    "    y_train = df[offer_id]['train']['target']\n",
    "    X_val = df[offer_id]['validation'].drop(columns=['target'])\n",
    "    y_val = df[offer_id]['validation']['target']\n",
    "\n",
    "    # Espaço de busca para a otimização Bayesiana dos hiperparâmetros do LightGBM\n",
    "    search_spaces = {\n",
    "        'learning_rate': Real(0.01, 0.2, prior='log-uniform'),\n",
    "        'num_leaves': Integer(20, 150),\n",
    "        'max_depth': Integer(3, 15),\n",
    "        'min_child_samples': Integer(10, 100),\n",
    "        'subsample': Real(0.5, 1.0),\n",
    "        'colsample_bytree': Real(0.5, 1.0),\n",
    "        'reg_alpha': Real(0.0, 1.0),\n",
    "        'reg_lambda': Real(0.0, 1.0)\n",
    "    }\n",
    "\n",
    "    # Inicializa o modelo LightGBM com alguns hiperparâmetros fixos\n",
    "    lgbm = LGBMClassifier(\n",
    "        objective='binary',\n",
    "        random_state=42,\n",
    "        class_weight='balanced',  # Para lidar com desbalanceamento da classe\n",
    "        n_estimators=500,\n",
    "        verbosity=-1  # Silencia os avisos\n",
    "    )\n",
    "\n",
    "    # Otimizador Bayesiano usando validação cruzada para encontrar os melhores hiperparâmetros\n",
    "    opt = BayesSearchCV(\n",
    "        estimator=lgbm,\n",
    "        search_spaces=search_spaces,\n",
    "        n_iter=30,            # Número de iterações da busca\n",
    "        cv=3,                 # Validação cruzada 3-fold\n",
    "        scoring='recall',     # Otimiza a sensibilidade (importante em casos de classe minoritária)\n",
    "        n_jobs=-1,            # Usa todos os núcleos disponíveis\n",
    "        verbose=0,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Executa a busca e treina o modelo com os melhores parâmetros\n",
    "    opt.fit(X_train, y_train)\n",
    "\n",
    "    # Recupera o melhor modelo encontrado\n",
    "    melhor_modelo = opt.best_estimator_\n",
    "\n",
    "    # Avalia o modelo na base de validação\n",
    "    y_pred = melhor_modelo.predict(X_val)\n",
    "\n",
    "    # Calcula métricas de avaliação\n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    sens = recall_score(y_val, y_pred)\n",
    "    cm = confusion_matrix(y_val, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.shape == (2, 2) else (0, 0, 0, 0)\n",
    "    especificidade = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "\n",
    "    # Exibe os melhores hiperparâmetros encontrados\n",
    "    print(\"\\nMelhores Hiperparâmetros encontrados:\")\n",
    "    print(opt.best_params_)\n",
    "\n",
    "    # Exibe as métricas de avaliação\n",
    "    print(f\"\\nAcurácia:      {acc:.6f}\")\n",
    "    print(f\"Sensibilidade: {sens:.6f}\")\n",
    "    print(f\"Especificidade: {especificidade:.6f}\")\n",
    "\n",
    "    # Reajusta o modelo com todos os dados de treino\n",
    "    melhor_modelo.fit(X_train, y_train)\n",
    "\n",
    "    # Define o caminho padrão para salvar os modelos\n",
    "    diretorio_modelos = \"./\"\n",
    "    os.makedirs(diretorio_dbfs, exist_ok=True)\n",
    "\n",
    "    caminho_completo = os.path.join(diretorio_modelos, f\"{nome_arquivo}.pkl\")\n",
    "\n",
    "    # Salva o modelo\n",
    "    with open(caminho_completo, \"wb\") as f:\n",
    "        pickle.dump(melhor_modelo, f)\n",
    "\n",
    "    print(f\"\\n Modelo salvo com sucesso em: {caminho_completo}\")\n",
    "\n",
    "    # Acessa o booster do modelo treinado\n",
    "    booster = melhor_modelo.booster_\n",
    "\n",
    "    # Plota a importância das features\n",
    "    lgb.plot_importance(booster, max_num_features=20, importance_type='split', figsize=(10, 6))\n",
    "    plt.title(\"Feature Importance (by split)\")\n",
    "    plt.show()\n",
    "\n",
    "    # Retorna o modelo treinado com os melhores hiperparâmetros\n",
    "    return melhor_modelo\n",
    "\n",
    "def ensemble_voto_por_score(\n",
    "    df_test: pd.DataFrame()\n",
    "):\n",
    "    '''\n",
    "    Info:\n",
    "        Carrega 9 modelos LightGBM do DBFS, faz predict_proba no conjunto de teste\n",
    "        e escolhe, para cada linha, a oferta correspondente ao modelo com maior score.\n",
    "    ----------\n",
    "    Input:\n",
    "        df_test: dataframe contendo dados de teste\n",
    "    ----------\n",
    "    Output:\n",
    "        y_pred_df: DataFrame com scores dos 9 modelos e a oferta escolhida por linha\n",
    "    '''\n",
    "\n",
    "    # Caminho base onde os modelos estão salvos\n",
    "    caminho_modelos = \"./\"\n",
    "\n",
    "    # Modelos e suas respectivas ofertas (índice i = modelo{i+1})\n",
    "    modelos_nomes = [f\"modelo{i}.pkl\" for i in range(1, 10)]\n",
    "    \n",
    "    ofertas = [\n",
    "        '2298d6c36e964ae4a3e7e9706d1fb8c2',\n",
    "        'ae264e3637204a6fb9bb56bc8210ddfd',\n",
    "        '9b98b8c7a33c4b65b9aebfe6a799e6d9',\n",
    "        'f19421c1d4aa40978ebb69ca19b0e20d',\n",
    "        '2906b810c7d4411798c6938adc9daaa5',\n",
    "        '0b1e1539f2cc45b7b9fa7c272da2e1d7',\n",
    "        'fafdcd668e3743c1bb461111dcafc2a4',\n",
    "        '4d5c57ea9a6940dd891ad53e9dbe8da0',\n",
    "        'No offer'\n",
    "    ]\n",
    "\n",
    "    resultados = {}\n",
    "\n",
    "    for nome_modelo, offer_id in zip(modelos_nomes, ofertas):\n",
    "        caminho_modelo = os.path.join(caminho_modelos, nome_modelo)\n",
    "\n",
    "        if not os.path.exists(caminho_modelo):\n",
    "            raise FileNotFoundError(f\"Modelo '{nome_modelo}' não encontrado em {caminho_modelo}.\")\n",
    "\n",
    "        # Carrega o modelo\n",
    "        with open(caminho_modelo, \"rb\") as f:\n",
    "            modelo = pickle.load(f)\n",
    "\n",
    "        # Dados de teste\n",
    "        X_test = df_test.drop(columns=['offer_id'])\n",
    "\n",
    "        # Probabilidade da classe positiva\n",
    "        y_prob = modelo.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # Usa o nome do modelo como chave temporária\n",
    "        resultados[nome_modelo] = y_prob\n",
    "\n",
    "    # Concatena as colunas com os nomes temporários (modelo1.pkl, etc)\n",
    "    y_pred_df = pd.DataFrame(resultados)\n",
    "\n",
    "    # Mapear modelo -> oferta\n",
    "    modelo_to_oferta = dict(zip(modelos_nomes, ofertas))\n",
    "\n",
    "    # Identifica qual modelo teve o maior score por linha\n",
    "    col_vencedora = y_pred_df.idxmax(axis=1)  # retorna modelo*.pkl\n",
    "\n",
    "    # Mapeia para o nome da oferta\n",
    "    y_pred_df['decisao_final'] = col_vencedora.map(modelo_to_oferta)\n",
    "\n",
    "    df_final = (\n",
    "        df_test\n",
    "        .reset_index(drop=True)\n",
    "        .merge(\n",
    "            y_pred_df, \n",
    "            how = 'left', \n",
    "            left_index=True, \n",
    "            right_index=True\n",
    "        )\n",
    "    )\n",
    "\n",
    "    acuracia_total = df_final.assign(acerto = lambda x: np.where(x.offer_id == x.decisao_final, 1, 0)).acerto.mean()\n",
    "\n",
    "    print(f'Acurácia do modelo: {np.round(100*acuracia_total, 3)}%')\n",
    "\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41d9e93e-6cfc-4a91-a459-9f42bc277b34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Carregando dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dde0f7de-bcc4-45fd-8b4c-e40ecd9db5e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"json\").load(\"ifood-case/data/processed/data_processed.json\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6024870-f384-42bb-80e3-f79f42f0eda5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Modelagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80a99b6c-a5a7-474c-9f8f-469df6baca84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e0ad603-a8f7-4abc-8d45-6a951ed15b8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Quantidade de ofertas na base\n",
    "offers = [c for c in list(df.offer_id.unique()) if c != None]\n",
    "num_offers = len(offers)\n",
    "num_offers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd3c3452-6ada-40f2-954a-61be12525f0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "offers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b97c072-efbe-46b6-a0c2-e1b546a499d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Gerando a base de treino, validacao e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81be0268-e8e5-403f-a278-cf109efeb4fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_train, df_val, df_test = cria_conjuntos(df=df)\n",
    "display(df_train.head(2))\n",
    "display(df_val.head(2))\n",
    "display(df_test.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "112cb7f3-ee95-45fd-bf24-7473185dd95a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Criando o sistema de classificação\n",
    "\n",
    "A metologia proposta consiste de:\n",
    "- Desenvolver um modelo de classificação para cada um dos 8 tipos de ofertas e um para detectar quando não tem oferta, aplicando a técnica One-vs-All, onde cada modelo vai ser treinado para identificar quando uma oferta em específico foi utilizada.\n",
    "- Ao todo serão treinados 9 modelos de classificação, um para cada oferta em específico e um para detectar não ofertas.\n",
    "- A oferta resultante da saída do sistema de classificação será aquela cujo score do modelo é o maior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5585e7e2-d35d-49bb-ae94-056a608be74c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c41e1bb9-0525-486c-b018-c5e0aa926322",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Criando um dicionario com os datasets tratados para cada modelo. Onde as chaves sao as ofertas.\n",
    "models_datasets = {}\n",
    "for offer in offers:\n",
    "    models_datasets[offer] = {}\n",
    "\n",
    "    # Obtendo os conjuntos de treino e validacao para cada modelo\n",
    "    for dataset, df_ in [('train', df_train), ('validation', df_val)]:\n",
    "        models_datasets[offer][dataset] = transform_data_for_model(df=df_, offer=offer)\n",
    "\n",
    "# Adicionando um modelo espeficico para detectar nao ofertas\n",
    "offer='No offer'\n",
    "models_datasets[offer] = {}\n",
    "for dataset, df_ in [('train', df_train), ('validation', df_val)]:\n",
    "        models_datasets[offer][dataset] = transform_data_for_model(df=df_, offer=offer, offers=offers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c3adba7-f6b8-4557-8413-7145bc69bcf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Chaves relacionadas aos tipos de oferta\n",
    "models_datasets.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "450438a4-6487-43bd-b50e-57906426d125",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "modelo1 = treinar_modelo_lgbm_bayes(models_datasets,'2298d6c36e964ae4a3e7e9706d1fb8c2','modelo1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "421e1439-1296-491d-962a-54d64c503423",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "modelo2 = treinar_modelo_lgbm_bayes(models_datasets,'ae264e3637204a6fb9bb56bc8210ddfd','modelo2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f13b676-99a8-4449-bb61-53dc8807dc04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "modelo3 = treinar_modelo_lgbm_bayes(models_datasets,'9b98b8c7a33c4b65b9aebfe6a799e6d9','modelo3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd39b044-aced-43ac-a290-29b8d1832ea7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "modelo4 = treinar_modelo_lgbm_bayes(models_datasets,'f19421c1d4aa40978ebb69ca19b0e20d','modelo4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44c483fd-5adf-4975-a23e-1dab86d78bbc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "modelo5 = treinar_modelo_lgbm_bayes(models_datasets,'2906b810c7d4411798c6938adc9daaa5','modelo5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87e92be4-2343-40f5-98a9-54264a28df37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "modelo6 = treinar_modelo_lgbm_bayes(models_datasets,'0b1e1539f2cc45b7b9fa7c272da2e1d7','modelo6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c2442e3-f19c-4c42-8372-75e58ce5fb5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "modelo7 = treinar_modelo_lgbm_bayes(models_datasets,'fafdcd668e3743c1bb461111dcafc2a4','modelo7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "084f2798-f82e-4507-90b8-d5e29dafeb95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "modelo8 = treinar_modelo_lgbm_bayes(models_datasets,'4d5c57ea9a6940dd891ad53e9dbe8da0','modelo8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46fee733-378c-4eb1-9256-010fb71f8b97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "modelo9 = treinar_modelo_lgbm_bayes(models_datasets,'No offer','modelo9')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f33d5b8c-3695-4bf1-ae8d-c044bf579e22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Avaliação "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7311dafa-00ca-47e1-b0a1-a5797666543d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Removendo colunas de ID e as que são relacionadas com o ID da oferta\n",
    "rem_cols = [\n",
    "    'account_id',\n",
    "    'discount_value',\n",
    "    'min_value',\n",
    "    'duration',\n",
    "    'num_channels',\n",
    "    'offer_type_bogo', 'offer_type_discount', 'offer_type_informational',\n",
    "    'channel_email', 'channel_mobile', 'channel_social', 'channel_web'\n",
    "]\n",
    "            \n",
    "df_test = df_test.drop(columns = rem_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b8a9212-34e5-447f-836d-66350c551dba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_resultados = ensemble_voto_por_score(df_test)\n",
    "df_resultados.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5421018d-d394-411a-84fe-03b79f48aa79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "amt_com_cupom = np.round(df_resultados[(df_resultados.decisao_final == df_resultados.offer_id) & (df_resultados.offer_id != 'No offer')].amount.mean(), 2)\n",
    "amt_sem_cupom = np.round(df_resultados[(df_resultados.decisao_final == df_resultados.offer_id) & (df_resultados.offer_id == 'No offer')].amount.mean(), 2)\n",
    "print(f\"Media do amount quando o usuario usa cupom: R${amt_com_cupom}\")\n",
    "print(f\"Media do amount quando o usuario nao usa cupom: R${amt_sem_cupom}\")\n",
    "print(f'Usuários que usam cupom gastam, em média, {np.round(100*((amt_com_cupom/amt_sem_cupom) - 1), 2)}% a mais')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30098d81-884a-4b62-97ad-72281d379589",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ganho em reais por ter aplicado o cupom efetivamente (considerando que o amount de compras sem cupom são, em média, 60% menores)\n",
    "amt_total_com_cupom = df_resultados[(df_resultados.decisao_final == df_resultados.offer_id) & (df_resultados.offer_id != 'No offer')].amount.sum()\n",
    "\n",
    "amt_total_sem_cupom = df_resultados[(df_resultados.decisao_final == df_resultados.offer_id) & (df_resultados.offer_id != 'No offer')].amount.sum() * 0.4\n",
    "\n",
    "print(f'Ganho monetário na aplicação de cupons segundo o sistema: R${np.round(amt_total_com_cupom - amt_total_sem_cupom, 2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8828c269-6aeb-4aa0-ae23-ba3af60219be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Salvando os resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1bf91108-05e0-4a90-a45b-a41787206855",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_resultados.to_csv(\"ifood-case/data/processed/3_results_model.csv\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "(Clone) 2_modeling",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}